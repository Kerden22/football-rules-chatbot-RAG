from __future__ import annotations  # Tip notlarÄ±nÄ± ertelemek iÃ§in
import os
import json
import uuid
import base64
import requests
from fastapi import FastAPI, HTTPException, Request
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates
from dotenv import load_dotenv
from pydantic import BaseModel
import uuid


# Pydantic modeller (models) burada tanÄ±mlandÄ±
class QueryRequest(BaseModel):
    question: str

class RenameRequest(BaseModel):
    title: str

# Ã‡evre deÄŸiÅŸkenlerini yÃ¼kle
load_dotenv()

# FastAPI uygulamasÄ±nÄ± baÅŸlat
app = FastAPI()
app.mount("/static", StaticFiles(directory="static"), name="static")
templates = Jinja2Templates(directory="templates")

# ------------------------------
# ChromaDB ve Google Gemini LLM AyarlarÄ± (main.py iÃ§inde yer alÄ±yor)
# ------------------------------
from langchain_community.document_loaders import PyPDFLoader  
from langchain_text_splitters import RecursiveCharacterTextSplitter  
from langchain_chroma import Chroma
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain.chains import create_retrieval_chain
from langchain_core.prompts import ChatPromptTemplate
from langchain.chains.combine_documents import create_stuff_documents_chain

print("ðŸ“– PDF yÃ¼kleniyor ve iÅŸleniyor...")
loader = PyPDFLoader("FutbolKurallarÄ±.pdf")  
data = loader.load()  
all_text = "\n".join([page.page_content for page in data])  
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)  
docs = text_splitter.split_text(all_text)

embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
vectorstore = Chroma.from_texts(docs, embeddings, persist_directory="./chroma_db")
retriever = vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 10})

llm = ChatGoogleGenerativeAI(
    model="gemini-1.5-pro",
    temperature=0.3,
    max_tokens=500
)

system_prompt = (
    "Sen bir yardÄ±mcÄ± asistansÄ±n ve yalnÄ±zca futbol kurallarÄ± hakkÄ±nda sorulara cevap veriyorsun. "
    "YanÄ±tlarÄ±nÄ± yalnÄ±zca verilen baÄŸlam iÃ§eriÄŸinden oluÅŸtur. "
    "EÄŸer sorunun cevabÄ±nÄ± bilmiyorsan, 'Bu konuda yardÄ±mcÄ± olamÄ±yorum.' de. "
    "CevaplarÄ±nÄ± en fazla Ã¼Ã§ cÃ¼mle ile ver ve doÄŸru bilgi iÃ§erdiÄŸinden emin ol.\n\n"
    "{context}"
)

prompt = ChatPromptTemplate.from_messages([
    ("system", system_prompt),
    ("human", "{input}")
])
question_answer_chain = create_stuff_documents_chain(llm, prompt)
rag_chain = create_retrieval_chain(retriever, question_answer_chain)

def get_rag_response(question: str) -> str:
    response = rag_chain.invoke({"input": question})
    return response["answer"]

# ------------------------------
# Chat History FonksiyonlarÄ±
# ------------------------------
SESSIONS_FILE = "chat_history.json"

def load_chat_history() -> dict:
    if not os.path.exists(SESSIONS_FILE):
        return {"sessions": []}
    with open(SESSIONS_FILE, "r", encoding="utf-8") as f:
        try:
            return json.load(f)
        except json.JSONDecodeError:
            return {"sessions": []}

def save_chat_history(data: dict):
    with open(SESSIONS_FILE, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

def find_session(data: dict, session_id: str):
    for session in data.get("sessions", []):
        if session["id"] == session_id:
            return session
    return None

# ------------------------------
# app/endpoint.py dosyasÄ±ndaki tÃ¼m Endpoint'leri uygulamaya dahil et
# ------------------------------
from app.endpoint import router as endpoints_router
app.include_router(endpoints_router)
