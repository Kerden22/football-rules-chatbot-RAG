import streamlit as st
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from dotenv import load_dotenv
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.chains import create_retrieval_chain
from langchain_core.prompts import ChatPromptTemplate
from langchain.chains.combine_documents import create_stuff_documents_chain

# ğŸ“Œ .env dosyasÄ±nÄ± yÃ¼kle
load_dotenv()

# ğŸ“Œ Streamlit baÅŸlÄ±k
st.title("âš½ Futbol KurallarÄ± Chatbot'u")

# ğŸ“Œ **PDF'yi ve vektÃ¶r veritabanÄ±nÄ± sadece bir kez yÃ¼kle**
if "vectorstore" not in st.session_state:
    with st.spinner("ğŸ“– PDF yÃ¼kleniyor ve iÅŸleniyor... (Sadece ilk seferde)"):
        loader = PyPDFLoader("FutbolKurallarÄ±.pdf")
        data = loader.load()
        
        # ğŸ“Œ PDF'yi tek bir metin haline getir
        all_text = "\n".join([page.page_content for page in data])

        # ğŸ“Œ Metni chunk'lara bÃ¶lme iÅŸlemi
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
        docs = text_splitter.split_text(all_text)

        # ğŸ“Œ Google Gemini Embedding modelini baÅŸlat
        embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")

        # ğŸ“Œ ChromaDB'yi baÅŸlat ve embedding iÅŸlemi yap
        vectorstore = Chroma.from_texts(texts=docs, embedding=embeddings, persist_directory="./chroma_db")

        # ğŸ“Œ ChromaDB tekrar yÃ¼kleme
        vectorstore = Chroma(persist_directory="./chroma_db", embedding_function=embeddings)

        # **YalnÄ±zca ilk Ã§alÄ±ÅŸtÄ±rmada yÃ¼kleyelim**
        st.session_state.vectorstore = vectorstore

# ğŸ“Œ Retriever oluÅŸtur (benzerlik aramasÄ± yapacak)
retriever = st.session_state.vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 10})

# ğŸ“Œ Google Gemini LLM'yi baÅŸlat
llm = ChatGoogleGenerativeAI(
    model="gemini-1.5-pro",
    temperature=0.3,
    max_tokens=500
)

# ğŸ“Œ Sistem Prompt'u
system_prompt = (
    "Sen bir yardÄ±mcÄ± asistansÄ±n ve yalnÄ±zca futbol kurallarÄ± hakkÄ±nda sorulara cevap veriyorsun. "
    "YanÄ±tlarÄ±nÄ± yalnÄ±zca verilen baÄŸlam iÃ§eriÄŸinden oluÅŸtur. "
    "EÄŸer sorunun cevabÄ±nÄ± bilmiyorsan, 'Bu konuda yardÄ±mcÄ± olamÄ±yorum.' de. "
    "CevaplarÄ±nÄ± en fazla Ã¼Ã§ cÃ¼mle ile ver ve doÄŸru bilgi iÃ§erdiÄŸinden emin ol.\n\n"
    "{context}"
)

# ğŸ“Œ Prompt Åablonu
prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system_prompt),
        ("human", "{input}")
    ]
)

# ğŸ“Œ Question-Answer zincirini oluÅŸtur
question_answer_chain = create_stuff_documents_chain(llm, prompt)

# ğŸ“Œ Retriever + LLM kombinasyonu ile RAG zincirini oluÅŸtur
rag_chain = create_retrieval_chain(retriever, question_answer_chain)

# ğŸ“Œ Sohbet geÃ§miÅŸini gÃ¶ster
if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.write(message["content"])

# ğŸ“Œ KullanÄ±cÄ±dan giriÅŸ al
query = st.chat_input("âš½ Futbol kurallarÄ± hakkÄ±nda bir soru sor:")

if query:
    # KullanÄ±cÄ±nÄ±n mesajÄ±nÄ± ekranda gÃ¶ster
    st.session_state.messages.append({"role": "user", "content": query})
    with st.chat_message("user"):
        st.write(query)

    with st.spinner("YanÄ±t hazÄ±rlanÄ±yor..."):
        response = rag_chain.invoke({"input": query})
        answer = response["answer"]

        # AsistanÄ±n cevabÄ±nÄ± ekranda gÃ¶ster
        st.session_state.messages.append({"role": "assistant", "content": answer})
        with st.chat_message("assistant"):
            st.write(answer)
