# Ã§alÄ±ÅŸtÄ±rmak iÃ§in : uvicorn api:app --reload    http://localhost:8000/

from fastapi import FastAPI
from pydantic import BaseModel
from langchain_community.document_loaders import PyPDFLoader  
from langchain_text_splitters import RecursiveCharacterTextSplitter  
from langchain_chroma import Chroma
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from dotenv import load_dotenv
from langchain.chains import create_retrieval_chain
from langchain_core.prompts import ChatPromptTemplate
from langchain.chains.combine_documents import create_stuff_documents_chain
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates
from fastapi import FastAPI, Request
import json
import os
import uuid
from typing import Optional
from fastapi import HTTPException

#  Ã‡evre deÄŸiÅŸkenlerini yÃ¼kle
load_dotenv()

#  FastAPI uygulamasÄ±nÄ± baÅŸlat
app = FastAPI()

#  Static ve Template dosyalarÄ±nÄ± baÄŸla
app.mount("/static", StaticFiles(directory="static"), name="static")
templates = Jinja2Templates(directory="templates")

#  Ana sayfa (HTML arayÃ¼zÃ¼)
@app.get("/")
def home(request: Request):
    return templates.TemplateResponse("index.html", {"request": request})

#  ChromaDB ve LLM modelini yÃ¼kleme
if "vectorstore" not in globals():
    print("ğŸ“– PDF yÃ¼kleniyor ve iÅŸleniyor...")

    # PDF'yi yÃ¼kle ve metin haline getir
    loader = PyPDFLoader("FutbolKurallarÄ±.pdf")  
    data = loader.load()  
    all_text = "\n".join([page.page_content for page in data])  

    # Metni chunk'lara bÃ¶l
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)  
    docs = text_splitter.split_text(all_text)  
    
    #  Google Gemini Embedding modelini baÅŸlat
    embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")

    # Chunk'larÄ± (docs) kullanarak ilk kez ChromaDB oluÅŸtur
    vectorstore = Chroma.from_texts(docs, embeddings, persist_directory="./chroma_db")

    #  Retriever oluÅŸtur 
    retriever = vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 10})

    #  Google Gemini LLM'yi baÅŸlat
    llm = ChatGoogleGenerativeAI(
        model="gemini-1.5-pro",
        temperature=0.3,
        max_tokens=500
    )

    #  Sistem Prompt'u
    system_prompt = (
        "Sen bir yardÄ±mcÄ± asistansÄ±n ve yalnÄ±zca futbol kurallarÄ± hakkÄ±nda sorulara cevap veriyorsun. "
        "YanÄ±tlarÄ±nÄ± yalnÄ±zca verilen baÄŸlam iÃ§eriÄŸinden oluÅŸtur. "
        "EÄŸer sorunun cevabÄ±nÄ± bilmiyorsan, 'Bu konuda yardÄ±mcÄ± olamÄ±yorum.' de. "
        "CevaplarÄ±nÄ± en fazla Ã¼Ã§ cÃ¼mle ile ver ve doÄŸru bilgi iÃ§erdiÄŸinden emin ol.\n\n"
        "{context}"
    )

    #  Prompt Åablonu
    prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system_prompt),
            ("human", "{input}")
        ]
    )

    #  Question-Answer zincirini oluÅŸtur
    question_answer_chain = create_stuff_documents_chain(llm, prompt)

    #  Retriever + LLM kombinasyonu ile RAG zincirini oluÅŸtur
    rag_chain = create_retrieval_chain(retriever, question_answer_chain)

#  KullanÄ±cÄ±dan gelen sorguyu tanÄ±mlamak iÃ§in bir model oluÅŸtur
class QueryRequest(BaseModel):
    question: str

#  Chatbot iÃ§in bir API endpoint'i oluÅŸtur
@app.post("/ask")
def ask_question(request: QueryRequest):
    query = request.question
    print(f"ğŸ“¨ Soru alÄ±ndÄ±: {query}")

    # YanÄ±tÄ± oluÅŸtur
    response = rag_chain.invoke({"input": query})
    answer = response["answer"]

    print(f"ğŸ¤– YanÄ±t: {answer}")
    return {"question": query, "answer": answer}

# -------------------------------------------------------------------
# Chat_History json iÃ§in kodlar
# -------------------------------------------------------------------

SESSIONS_FILE = "chat_history.json"

def load_chat_history():
    """
    chat_history.json dosyasÄ±nÄ± okuyup sÃ¶zlÃ¼k (dict) olarak dÃ¶ndÃ¼rÃ¼r.
    EÄŸer dosya yoksa veya dosya boÅŸ/geÃ§ersizse, 'sessions': [] iÃ§eren bir sÃ¶zlÃ¼k oluÅŸturur.
    """
    if not os.path.exists(SESSIONS_FILE):
        return {"sessions": []}
    with open(SESSIONS_FILE, "r", encoding="utf-8") as f:
        try:
            return json.load(f)
        except json.JSONDecodeError:
            return {"sessions": []}

def save_chat_history(data: dict):
    """
    Verilen sÃ¶zlÃ¼ÄŸÃ¼ chat_history.json dosyasÄ±na yazar.
    """
    with open(SESSIONS_FILE, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

def find_session(data: dict, session_id: str) -> Optional[dict]:
    """
    Verilen data iÃ§indeki sessions listesinde session_id eÅŸleÅŸen oturumu dÃ¶ndÃ¼rÃ¼r.
    Bulunamazsa None dÃ¶ner.
    """
    for session in data["sessions"]:
        if session["id"] == session_id:
            return session
    return None

def get_rag_response(question: str) -> str:
    """
    RAG zincirinden (rag_chain) bir cevap alÄ±r ve sadece metin cevabÄ± dÃ¶ndÃ¼rÃ¼r.
    """
    response = rag_chain.invoke({"input": question})
    return response["answer"]

@app.get("/sessions")
def list_sessions():
    """
    KayÄ±tlÄ± tÃ¼m sohbet oturumlarÄ±nÄ± (id ve title) dÃ¶ndÃ¼rÃ¼r.
    """
    data = load_chat_history()
    # Her oturumun id ve title alanÄ±nÄ± dÃ¶nÃ¼yoruz
    return [{"id": s["id"], "title": s["title"]} for s in data["sessions"]]

@app.get("/sessions/{session_id}")
def get_session(session_id: str):
    """
    Belirli bir oturumun tÃ¼m mesajlarÄ±nÄ± dÃ¶ndÃ¼rÃ¼r.
    """
    data = load_chat_history()
    session = find_session(data, session_id)
    if not session:
        raise HTTPException(status_code=404, detail="Session not found")
    return session

@app.post("/sessions")
def create_session(request: QueryRequest):
    """
    Yeni bir sohbet oturumu oluÅŸturur.
    request.question -> Ä°lk kullanÄ±cÄ± mesajÄ±
    Bot cevabÄ± da eklenir. SonuÃ§ta oluÅŸan oturum geri dÃ¶ndÃ¼rÃ¼lÃ¼r.
    """
    data = load_chat_history()

    # Yeni session oluÅŸtur
    session_id = str(uuid.uuid4())
    user_msg = request.question
    bot_msg = get_rag_response(user_msg)

    new_session = {
        "id": session_id,
        "title": user_msg,  # Oturumun baÅŸlÄ±ÄŸÄ± ilk sorudan
        "messages": [
            {"role": "user", "content": user_msg},
            {"role": "assistant", "content": bot_msg}
        ]
    }

    data["sessions"].append(new_session)
    save_chat_history(data)

    return new_session

@app.post("/sessions/{session_id}/messages")
def add_message(session_id: str, request: QueryRequest):
    """
    Mevcut oturuma yeni bir kullanÄ±cÄ± mesajÄ± ekler,
    RAG zincirinden bot cevabÄ±nÄ± alÄ±r ve onu da ekler.
    """
    data = load_chat_history()
    session = find_session(data, session_id)
    if not session:
        raise HTTPException(status_code=404, detail="Session not found")

    user_msg = request.question
    bot_msg = get_rag_response(user_msg)

    # MesajlarÄ± ekle
    session["messages"].append({"role": "user", "content": user_msg})
    session["messages"].append({"role": "assistant", "content": bot_msg})

    # DosyayÄ± gÃ¼ncelle
    save_chat_history(data)

    # Bot cevabÄ±nÄ± dÃ¶ndÃ¼rÃ¼yoruz
    return {"question": user_msg, "answer": bot_msg}

@app.delete("/sessions/{session_id}")
def delete_session(session_id: str):
    data = load_chat_history()
    session = find_session(data, session_id)
    if not session:
        raise HTTPException(status_code=404, detail="Session not found")

    data["sessions"].remove(session)
    save_chat_history(data)

    return {"status": "success", "id": session_id}

class RenameRequest(BaseModel):
    title: str

@app.patch("/sessions/{session_id}")
def rename_session(session_id: str, request: RenameRequest):
    """
    Belirli bir oturumun baÅŸlÄ±ÄŸÄ±nÄ± (title) deÄŸiÅŸtirir.
    """
    data = load_chat_history()
    session = find_session(data, session_id)
    if not session:
        raise HTTPException(status_code=404, detail="Session not found")

    session["title"] = request.title
    save_chat_history(data)
    return {"status": "renamed", "id": session_id, "newTitle": request.title}
