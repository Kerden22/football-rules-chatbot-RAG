# Ã§alÄ±ÅŸtÄ±rmak iÃ§in : uvicorn api:app --reload    http://localhost:8000/

from fastapi import FastAPI
from pydantic import BaseModel
from langchain_chroma import Chroma
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from dotenv import load_dotenv
from langchain.chains import create_retrieval_chain
from langchain_core.prompts import ChatPromptTemplate
from langchain.chains.combine_documents import create_stuff_documents_chain
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates
from fastapi import FastAPI, Request

#  Ã‡evre deÄŸiÅŸkenlerini yÃ¼kle
load_dotenv()

#  FastAPI uygulamasÄ±nÄ± baÅŸlat
app = FastAPI()

#  Static ve Template dosyalarÄ±nÄ± baÄŸla
app.mount("/static", StaticFiles(directory="static"), name="static")
templates = Jinja2Templates(directory="templates")

#  Ana sayfa (HTML arayÃ¼zÃ¼)
@app.get("/")
def home(request: Request):
    return templates.TemplateResponse("index.html", {"request": request})

#  ChromaDB ve LLM modelini yÃ¼kleme
if "vectorstore" not in globals():
    print("ğŸ“– PDF yÃ¼kleniyor ve iÅŸleniyor...")
    
    #  Google Gemini Embedding modelini baÅŸlat
    embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")

    #  ChromaDB'yi baÅŸlat ve yÃ¼kle
    vectorstore = Chroma(persist_directory="./chroma_db", embedding_function=embeddings)

    #  Retriever oluÅŸtur 
    retriever = vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 10})

    #  Google Gemini LLM'yi baÅŸlat
    llm = ChatGoogleGenerativeAI(
        model="gemini-1.5-pro",
        temperature=0.3,
        max_tokens=500
    )

    #  Sistem Prompt'u
    system_prompt = (
        "Sen bir yardÄ±mcÄ± asistansÄ±n ve yalnÄ±zca futbol kurallarÄ± hakkÄ±nda sorulara cevap veriyorsun. "
        "YanÄ±tlarÄ±nÄ± yalnÄ±zca verilen baÄŸlam iÃ§eriÄŸinden oluÅŸtur. "
        "EÄŸer sorunun cevabÄ±nÄ± bilmiyorsan, 'Bu konuda yardÄ±mcÄ± olamÄ±yorum.' de. "
        "CevaplarÄ±nÄ± en fazla Ã¼Ã§ cÃ¼mle ile ver ve doÄŸru bilgi iÃ§erdiÄŸinden emin ol.\n\n"
        "{context}"
    )

    #  Prompt Åablonu
    prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system_prompt),
            ("human", "{input}")
        ]
    )

    #  Question-Answer zincirini oluÅŸtur
    question_answer_chain = create_stuff_documents_chain(llm, prompt)

    #  Retriever + LLM kombinasyonu ile RAG zincirini oluÅŸtur
    rag_chain = create_retrieval_chain(retriever, question_answer_chain)

#  KullanÄ±cÄ±dan gelen sorguyu tanÄ±mlamak iÃ§in bir model oluÅŸtur
class QueryRequest(BaseModel):
    question: str

#  Chatbot iÃ§in bir API endpoint'i oluÅŸtur
@app.post("/ask")
def ask_question(request: QueryRequest):
    query = request.question
    print(f"ğŸ“¨ Soru alÄ±ndÄ±: {query}")

    # YanÄ±tÄ± oluÅŸtur
    response = rag_chain.invoke({"input": query})
    answer = response["answer"]

    print(f"ğŸ¤– YanÄ±t: {answer}")
    return {"question": query, "answer": answer}
